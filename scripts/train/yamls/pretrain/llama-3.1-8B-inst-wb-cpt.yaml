variables:
  data_local: /home/riccar_orlando/data
  data_remote:  # If blank, files must be present in data_local
  max_seq_len: 8192
  global_seed: 17
  model_name: meta-llama/Llama-3.1-8B-Instruct
  tokenizer_name: meta-llama/Llama-3.1-8B-Instruct
  # Run Name
  run_name:  llama3-8b-instruct-cpt-10B-dolmino-pdfs

max_seq_len: ${variables.max_seq_len}
run_name: ${variables.run_name}

# Model
model:
  name: hf_causal_lm
  init_device: cpu
  pretrained_model_name_or_path: ${variables.model_name}
  pretrained: true
  # Note: you must have set the HF_TOKEN environment variable and have access to the llama2 models
  use_auth_token: true
  use_flash_attention_2: true

# Tokenizer
tokenizer:
  name: ${variables.tokenizer_name}
  kwargs:
    model_max_length: ${variables.max_seq_len}

# Dataloaders
train_loader:
  name: text
  dataset:
    shuffle: true
    max_seq_len: ${variables.max_seq_len}
    shuffle_seed: ${variables.global_seed}
    streams:
      # dolmino 5B
      dclm: 
        # current available tokens: 23_600_000_000
        # needed tokens: 4_720_000_000
        local: ${variables.data_local}/dolmino-mix-1124-50B/processed/dclm
        split: train
        repeat: 0.1
      flan: # current available tokens: 8_300_000_000
        local: ${variables.data_local}/dolmino-mix-1124-50B/processed/flan
        split: train
        repeat: 0.1
      math: # current available tokens: 10400000000
        local: ${variables.data_local}/dolmino-mix-1124-50B/processed/math
        split: train
        repeat: 0.1
      pes2o: # current available tokens: 2925000000
        local: ${variables.data_local}/dolmino-mix-1124-50B/processed/pes2o
        split: train
        repeat: 0.1
      stackexchange: # current available tokens: 1225000000
        local: ${variables.data_local}/dolmino-mix-1124-50B/processed/stackexchange
        split: train
        repeat: 0.1
      wiki: # current available tokens: 3555000000
        local: ${variables.data_local}/dolmino-mix-1124-50B/processed/wiki
        split: train
        repeat: 0.1
      # pdfs 5B
      pdfs: # current available tokens: 8_853_544_960
        local: ${variables.data_local}/semantic-scholar-parsed-jsonl-en-processed
        split: train
        repeat: 1.77
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: true
  drop_last: true
  num_workers: 8

eval_loader:
  name: text
  dataset:
    shuffle: false
    max_seq_len: ${variables.max_seq_len}
    shuffle_seed: ${variables.global_seed}
    streams:
      # dolmino
      dclm:
        local: ${variables.data_local}/dolmino-mix-1124-50B/processed/dclm
        split: val
        repeat: 0.5
      flan:
        local: ${variables.data_local}/dolmino-mix-1124-50B/processed/flan
        split: val
        repeat: 0.5
      math:
        local: ${variables.data_local}/dolmino-mix-1124-50B/processed/math
        split: val
        repeat: 0.5
      pes2o:
        local: ${variables.data_local}/dolmino-mix-1124-50B/processed/pes2o
        split: val
        repeat: 0.5
      stackexchange:
        local: ${variables.data_local}/dolmino-mix-1124-50B/processed/stackexchange
        split: val
        repeat: 0.5
      wiki:
        local: ${variables.data_local}/dolmino-mix-1124-50B/processed/wiki
        split: val
        repeat: 0.5
      # pdfs
      pdfs:
        local: ${variables.data_local}/semantic-scholar-parsed-jsonl-en-processed
        split: val
        repeat: 0.25
  drop_last: false
  num_workers: 8

# Optimization
scheduler:
  name: cosine_with_warmup
  t_warmup: 100ba
  alpha_f: 0.1

optimizer:
  name: adamw
  lr: 3.0e-4
  betas:
  - 0.9
  - 0.95
  eps: 1.0e-08
  weight_decay: 0.1

algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0

# max_duration = num_tokens / (max_seq_len * global_train_batch_size)
# number of tokens per batch = max_seq_len * global_train_batch_size
max_duration: 2400ba
eval_interval: 240ba
eval_first: false
global_train_batch_size: 512

# System
seed: ${variables.global_seed}
device_train_microbatch_size: 4
precision: amp_bf16
dist_timeout: 6000
expandable_segments: true

# FSDP
fsdp_config:
  mixed_precision: PURE
  state_dict_type: sharded
  limit_all_gathers: true
  sharding_strategy: FULL_SHARD # HYBRID_SHARD
  backward_prefetch: BACKWARD_PRE
  activation_cpu_offload: false
  activation_checkpointing: false
  activation_checkpointing_reentrant: false

# Logging
progress_bar: false
log_to_console: true
console_log_interval: 1ba

callbacks:
  speed_monitor:
    window_size: 10
  lr_monitor: {}
  memory_monitor: {}
  runtime_estimator: {}

loggers:
  wandb:
    entity: "riccorl"
    project: "wb-llm-cpt"
    init_kwargs:
      mode: "offline"
      id: ${run_name}
      dir: /home/riccar_orlando/training_logs/{run_name}

# Checkpoint to local filesystem or remote object store
save_interval: 240ba
save_num_checkpoints_to_keep: -1 # Important, this cleans up checkpoints saved to DISK
save_folder: /home/riccar_orlando/runs/{run_name}
# save_folder: s3://my-bucket/my-folder/{run_name}/checkpoints

# Load from local filesystem or remote object store
# load_path: /home/riccar_orlando/runs/{run_name}/latest-rank{rank}.pt
# load_path: /leonardo_scratch/large/userexternal/rorland1/llm-foundry/runs/{run_name}/ep0-ba10000-rank0.pt
# load_path: s3://my-bucket/my-folder/gpt-125m/checkpoints/latest-rank{rank}.pt
autoresume: False

# ICL section
icl_tasks:
-
  label: arc_easy
  dataset_uri: ../eval/local_data/world_knowledge/arc_easy.jsonl
  num_fewshot: [3]
  icl_task_type: multiple_choice
  continuation_delimiter: "\nAnswer: "
-
  label: arc_challenge
  dataset_uri: ../eval/local_data/world_knowledge/arc_challenge.jsonl
  num_fewshot: [3, 25]
  icl_task_type: multiple_choice
  continuation_delimiter: "\nAnswer: "
-
  label: hellaswag
  dataset_uri: ../eval/local_data/language_understanding/hellaswag.jsonl  # ADD YOUR OWN DATASET URI
  num_fewshot: [10]
  icl_task_type: multiple_choice
