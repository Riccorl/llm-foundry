variables:
  data_local: /home/riccar_orlando/data
  data_remote:  # If blank, files must be present in data_local
  max_seq_len: 8192
  global_seed: 17
  model_name: yujiepan/meta-llama-3-tiny-random
  tokenizer_name: yujiepan/meta-llama-3-tiny-random
  # Run Name
  run_name:  llama3-1b-cpt-dolmino-10B-test

max_seq_len: ${variables.max_seq_len}
run_name: ${variables.run_name}

# Model
model:
  name: hf_causal_lm
  init_device: meta
  pretrained_model_name_or_path: ${variables.model_name}
  pretrained: true
  # Note: you must have set the HF_TOKEN environment variable and have access to the llama2 models
  use_auth_token: true
  use_flash_attention_2: true
  # attn_implementation: flash_attention_2

# Tokenizer
tokenizer:
  name: ${variables.tokenizer_name}
  kwargs:
    model_max_length: ${variables.max_seq_len}

# Dataloaders
train_loader:
  name: text
  dataset:
    # local: ${variables.data_local}
    # remote: ${variables.data_remote}
    # split: train_small
    shuffle: true
    max_seq_len: ${variables.max_seq_len}
    shuffle_seed: ${variables.global_seed}
    streams:
      # dolmino dclm
      dclm:
        local: ${variables.data_local}/dolmino-mix-1124-50B/processed/dclm
        split: train
        repeat: 1.0
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: true
  drop_last: true
  num_workers: 8

eval_loader:
  name: text
  dataset:
    # local: ${variables.data_local}
    # remote: ${variables.data_remote}
    # split: val
    shuffle: false
    max_seq_len: ${variables.max_seq_len}
    shuffle_seed: ${variables.global_seed}
    streams:
      # dolmino dclm
      dclm:
        local: ${variables.data_local}/dolmino-mix-1124-50B/processed/dclm
        split: val
        repeat: 1.0
  drop_last: false
  num_workers: 8

# Optimization
scheduler:
  name: cosine_with_warmup
  t_warmup: 100ba
  alpha_f: 0.1

optimizer:
  name: adamw
  lr: 3.0e-4
  betas:
  - 0.9
  - 0.95
  eps: 1.0e-08
  weight_decay: 0.1

algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0

# max_duration = num_tokens / (max_seq_len * global_train_batch_size)
# 2384 / 4194304
# number of tokens per batch = max_seq_len * global_train_batch_size
max_duration: 2400ba
eval_interval: 500ba
eval_first: false
global_train_batch_size: 512

# System
seed: ${variables.global_seed}
device_train_microbatch_size: 1
precision: amp_bf16
dist_timeout: 6000
expandable_segments: true

# FSDP
# fsdp_config:
#   mixed_precision: PURE
#   state_dict_type: sharded
#   limit_all_gathers: true
#   sharding_strategy: FULL_SHARD # HYBRID_SHARD
#   backward_prefetch: BACKWARD_PRE
#   activation_cpu_offload: false
#   activation_checkpointing: false
#   activation_checkpointing_reentrant: false

# Logging
progress_bar: false
log_to_console: true
console_log_interval: 1ba

callbacks:
  speed_monitor:
    window_size: 10
  lr_monitor: {}
  memory_monitor: {}
  runtime_estimator: {}

loggers:
  wandb:
    entity: "riccorl"
    project: "wb-llm-cpt"
    init_kwargs:
      mode: "offline"
      # id: ${run_name}
      dir: /home/riccar_orlando/training_logs/{run_name}

# Checkpoint to local filesystem or remote object store
save_interval: 500ba
save_num_checkpoints_to_keep: -1 # Important, this cleans up checkpoints saved to DISK
save_folder: /home/riccar_orlando/runs/{run_name}
# save_folder: s3://my-bucket/my-folder/{run_name}/checkpoints

# Load from local filesystem or remote object store
# load_path: /home/riccar_orlando/runs/{run_name}/latest-rank{rank}.pt
# load_path: /leonardo_scratch/large/userexternal/rorland1/llm-foundry/runs/{run_name}/ep0-ba10000-rank0.pt
# load_path: s3://my-bucket/my-folder/gpt-125m/checkpoints/latest-rank{rank}.pt
autoresume: False

# ICL section
icl_tasks:
-
  label: arc_easy
  dataset_uri: ../eval/local_data/world_knowledge/arc_easy.jsonl
  num_fewshot: [3]
  icl_task_type: multiple_choice
  continuation_delimiter: "\nAnswer: "
-
  label: arc_challenge
  dataset_uri: ../eval/local_data/world_knowledge/arc_challenge.jsonl
  num_fewshot: [3, 25]
  icl_task_type: multiple_choice
  continuation_delimiter: "\nAnswer: "
-
  label: mmlu
  dataset_uri: ../eval/local_data/world_knowledge/mmlu.jsonl
  num_fewshot: [5]
  icl_task_type: multiple_choice
  continuation_delimiter: "\nAnswer: "
  has_categories: true
